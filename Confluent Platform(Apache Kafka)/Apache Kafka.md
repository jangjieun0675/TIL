# Kafka 개요

## 1. Kafka란 무엇인가?
- 아파치 소프트웨어 재단에서 개발한 **분산 이벤트 스트리밍 플랫폼**  
- 데이터를 **빠르게 수집**하고 **처리**하는 데 특화된 시스템  
- 주로 **대용량 데이터 처리**, **실시간 분석**, **마이크로서비스 통신** 등에 사용  
- 데이터 흐름을 최적화하고, **실시간 데이터 기반 의사결정**을 강화할 수 있다  

---

## 2. Kafka의 핵심기능과 특징

### ● 이벤트 스트리밍(Event Streaming)
- **이벤트 스트림**은 결제, 주문, 데이터베이스 변경 등과 같은 **실시간 이벤트**를 의미  
- Kafka는 이러한 이벤트를 **밀리초 단위**로 생성하고 수집하며, 다양한 시스템과 공유할 수 있다  
- 여러 서비스가 생성한 데이터를 **빠르고 실시간으로 전달**하는 역할을 한다  

### ● 필요한 만큼 보관 및 기록 재처리(데이터 보존)
- Kafka는 수집된 데이터를 **필요한 기간 동안 보관**할 수 있다  
- 데이터를 단순히 저장하는 것을 넘어서, **과거 데이터를 재생**해 특정 시점의 이벤트를 다시 처리할 수 있다  
- **데이터 손실 없이 재처리**가 가능해 유용하다  

### ● 지속적인 실시간 처리
- Kafka를 사용하면 **실시간 이벤트**에 즉각적으로 대응하는 애플리케이션을 개발할 수 있다  
- 확장 가능한 **가볍고 탄력적인 애플리케이션**을 구축할 수 있어 **마이크로서비스 환경**에 적합  
- 이벤트 발생과 동시에 처리할 수 있어 **실시간 작업**을 지원  

### ● 데이터 스트림과 테이블의 통합 및 분석
- Kafka는 **데이터 스트림과 정적 테이블 데이터**를 실시간으로 결합하여 처리할 수 있다  
- **24시간 중단 없이** 처리와 분석이 가능하며, 실시간 의사 결정을 지원  
- 이를 통해 **데이터 기반 의사 결정**이 신속하게 이루어질 수 있다  

### ● 데이터의 유연한 이동과 연결(어디서나 사용 가능)
- Kafka는 기업 내부와 외부의 모든 시스템을 연결하는 **데이터 이동 플랫폼**으로 활용된다  
- 데이터 센터, 시스템, **클라우드 간 데이터 이동**을 원활하게 지원  
- Kafka는 동일한 기술로 **클라우드 환경과 온프레미스(On-premise) 환경**에서 모두 신뢰성 있게 사용될 수 있다  
  - **온프레미스(On-premise) 환경**: 서버를 직접 설치·운영하는 방식으로, 원격 클라우드 운영과 대비되는 개념  

### ● 초대규모 처리 능력
- Kafka는 **매우 큰 규모의 데이터를 안정적으로 처리**할 수 있다  
- 일일 **수조 개의 이벤트**를 처리할 수 있으며, 기업의 프로덕션 환경에서도 **안정적인 성능**을 제공한다  

---

## 3. Kafka의 주요 사용 사례
- **실시간 금융거래 처리**: 결제나 주문 데이터를 실시간으로 처리해 고객에게 즉각적인 피드백 제공  
- **로그 및 모니터링 데이터 수집**: 시스템 로그를 실시간으로 수집하고 분석해 문제를 신속하게 감지  
- **IoT(사물인터넷) 데이터 처리**: 센서와 기기에서 발생하는 데이터를 실시간으로 수집하고 분석  
- **마이크로서비스 통신**: 여러 마이크로서비스 간 **비동기 메시지 전송**을 통해 효율적인 통신 구현  

---

## 4. Kafka의 아키텍처 구성요소
Kafka는 **스토리지 계층**과 **컴퓨팅 계층**으로 나누어져 실시간 데이터 처리를 지원한다.

### ① 스토리지 계층(Storage Layer)
- **역할**: 데이터를 효율적으로 저장하고 보관  
- **확장성**: Kafka는 **분산 시스템**으로, 데이터 양이 증가할 때 **서버(브로커)**를 추가해 확장 가능  
- **데이터 보관 방식**: 데이터는 **토픽(Topic)** 단위로 저장되며, 각 토픽은 **파티션(Partition)**으로 나눠져 성능을 높인다  
  - 예: 결제 시스템이 Kafka를 사용해 거래 기록을 저장할 때 데이터가 많아지면, 서버를 추가해 처리 가능  

### ② 컴퓨팅 계층(Computing Layer)
Kafka는 **데이터 처리**와 **애플리케이션 간 통신**을 위한 4가지 구성 요소로 이루어져 있다.  

#### ⓐ Producer(생산자)
- **데이터 생성 후 Kafka로 전송**  
- 여러 서비스가 생성한 데이터를 Kafka **토픽으로 전송**  
  - 예: 쇼핑몰 결제 완료 시, 결제 정보를 Kafka로 전달  

#### ⓑ Consumer(소비자)
- **Kafka에 저장된 데이터를 사용**  
- 특정 토픽의 데이터를 **구독(subscribe)**하여 실시간으로 처리  
  - 예: 결제 발생 시 소비자 애플리케이션이 데이터를 받아 고객에게 알림 전송  

#### ⓒ Stream API(스트림 API)
- **실시간 데이터 처리 및 분석** 도구  
- 여러 이벤트를 **조합하거나 변환**하여 처리 가능  
  - 예: 결제 데이터와 주문 데이터를 결합해 실시간 매출 분석 수행  

#### ⓓ Connector API(커넥터 API)
- Kafka를 **외부 시스템과 연결**해 데이터 송수신 가능  
- **데이터베이스, 클라우드 서비스** 등과 연동에 사용  
  - 예: 결제 데이터가 데이터베이스에 자동 업데이트될 때 Kafka 활용  

---

## 5. ksqlDB(SQL을 활용한 스트리밍 데이터베이스)
- **역할**: Kafka Streams 기반의 **스트리밍 데이터베이스**로, 복잡한 코딩 없이 **SQL과 유사한 구문**으로 실시간 데이터를 처리합니다.  
- **기능**: SELECT, WHERE, JOIN과 같은 **SQL 구문**을 사용해 데이터를 조회하고 처리할 수 있습니다.  

**예시**:  
최근 1시간 동안 발생한 주문 수를 실시간으로 조회하는 ksqlDB 쿼리:  
```sql
SELECT COUNT(*) 
FROM orders_stream 
WHERE order_time > NOW() - INTERVAL '1' HOUR;
```

---

## 6. Kafka의 데이터 처리 흐름
1. **Producer**가 결제와 같은 이벤트를 생성해 Kafka로 전송  
2. Kafka의 **스토리지 계층**이 데이터를 **토픽과 파티션**으로 나눠 저장  
3. **Consumer**가 특정 토픽을 구독하고 데이터를 실시간으로 가져옴  
4. 필요한 경우 **Stream API**를 사용해 여러 데이터를 조합하여 처리  
5. **Connector API**로 **데이터베이스나 클라우드**와 데이터를 주고받음  

---

## 7. Kafka 아키텍처의 장점
- **확장성**: 서버(브로커) 추가로 데이터 양이 많아져도 손쉽게 확장 가능  
- **고성능**: 데이터를 파티션으로 나눠 여러 서버에서 동시에 처리해 성능 향상  
- **내구성**: Kafka에 저장된 데이터는 필요 시 **재생 및 재처리** 가능  
- **다양한 연동성**: **Connector API**로 여러 시스템과 손쉽게 통합 가능  

---

## 8. Kafka의 토픽(Topic)과 토픽 파티션(Topic Partitions) 개요
Kafka는 데이터를 **효율적이고 확장 가능**하게 저장 및 처리하기 위해 **토픽(Topic)**과 **파티션(Partition)** 개념을 사용합니다. 이 두 개념은 **실시간 데이터 처리와 병렬 처리**를 가능하게 하는 중요한 요소입니다.

### ① Topic(토픽) - Kafka의 기본 데이터 저장소
- **Topic**은 **이벤트(데이터)**를 저장하는 **기본 단위**입니다.  
- 마치 **파일 폴더**처럼 특정 주제의 데이터가 하나의 토픽에 저장됩니다.

#### **※ Topic의 특징**
- **추가 전용 불변 이벤트 로그**: 기록된 데이터는 수정되지 않고 **계속 추가**됩니다.  
- **파티션을 통해 데이터 처리 분산**: 하나의 **토픽**은 여러 개의 **파티션**으로 나누어 처리됩니다.  
- **여러 브로커(서버)에 분산**: 각 파티션은 **브로커(서버)**에 분산되어 효율적으로 처리됩니다.  

**예시**:  
- 쇼핑몰의 **결제 데이터**는 `payment`라는 토픽에, **고객 주문 정보**는 `orders`라는 별도 토픽에 저장됩니다.

### ② Partition(파티션) - 병렬 처리를 위한 기본 단위
- **Partition**은 **토픽을 작은 단위**로 나눈 것입니다.  
- 파티션은 **병렬 처리와 확장성**을 높이는 핵심 구성 요소입니다.

#### **※ Partition의 특징**
- **Commit Log 형태로 저장**: 파티션은 커밋 로그처럼 동작하며 데이터가 **순차적으로 추가**됩니다.  
- **유니크한 Offset(오프셋)**: 각 데이터는 **오프셋(Offset)**이라는 고유 식별자를 가지며, **소비자(Consumer)**는 이 오프셋을 통해 데이터를 어디까지 읽었는지 추적할 수 있습니다.  
- **병렬 처리 및 Throughput 향상**: 하나의 토픽이 여러 파티션으로 나눠질 경우, 각 파티션은 **다른 브로커에 배치**됩니다. 이로 인해 여러 클라이언트(Producer/Consumer)가 **동시에 데이터에 접근**해 병렬 처리가 가능합니다.  
- **확장성과 고가용성**: 파티션이 여러 브로커에 분산되어 저장되므로, **클러스터에 노드를 추가**하면 쉽게 확장할 수 있습니다. 또한, 데이터가 분산되어 하나의 서버에 문제가 발생해도 **다른 서버에서 데이터를 처리**해 고가용성을 보장합니다.

### ③ Topic과 Partition의 관계
- 하나의 **토픽**은 여러 **파티션**으로 구성됩니다.  
- 각 **브로커**는 특정 파티션의 **리더**가 되어 데이터를 효율적으로 분산 처리할 수 있습니다.

### ④ Partition을 통한 데이터 분산 처리 흐름
1. **Producer**가 주문 데이터를 `orders` 토픽에 전송합니다.  
2. 데이터는 `orders` 토픽의 **파티션** 중 하나에 할당됩니다.  
3. 각 파티션은 **오프셋**을 통해 데이터의 순서를 추적합니다.  
4. **Consumer**는 여러 파티션을 병렬로 구독하여 데이터를 **동시에 읽습니다**.  
5. 여러 브로커에 파티션이 분산되어 있어 여러 클라이언트가 **동시에 데이터에 접근**할 수 있습니다.

### ⑤ Kafka의 Topic과 Partition 구조의 장점
- **Throughput(처리량) 향상**: 여러 파티션에 데이터를 분산하여 **병렬 처리**로 응답 속도가 빨라집니다.  
- **확장성**: 데이터 양이 늘어나면 **파티션 수를 늘리고 브로커를 추가**하여 시스템을 확장할 수 있습니다.  
- **고가용성**: 데이터가 여러 서버에 분산 저장되므로 **한 서버가 장애**를 일으켜도 다른 서버가 **백업 역할**을 수행합니다.  
- **세분화된 데이터 처리**: 각 Consumer는 **원하는 파티션만 구독**할 수 있어 더 **정교한 데이터 처리**가 가능합니다.

---

## 9. 레코드 스키마(Record Schema)
* **스키마**: 데이터가 어떤 모양이어야 하는지를 정의  
* **레코드**: 파티션에 순서대로 저장됨  
* **오프셋**: 그 순서를 추적하는 역할  

- Kafka에서 데이터를 레코드(record) 형태로 저장하고 처리
- Kafka의 메시지는 직렬화된 데이터로 저장되기 때문에, 메시지를 해석하기 위해 **스키마**가 필요
- **Schema Registry**를 통해 스키마를 등록하고 관리하며, 이를 통해 메시지를 올바르게 인코딩, 디코딩할 수 있음
- **레코드**는 한 개의 이벤트나 메시지를 나타내며, **key, value, timestamp, headers**로 구성됨

### ① 레코드의 구성요소
- **Magic Byte**(0번째 바이트): Confluent 직렬화 형식 버전 번호, 항상 0으로 시작
- **Schema ID**(1~4번째 바이트): Schema Registry에서 관리하는 스키마 ID
- **Data**(5번째 바이트부터 끝까지): 실질적인 데이터가 들어가는 부분, 사전에 정의된 스키마에 따라 직렬화되어 저장

### ② Kafka에서 레코드 흐름
- Kafka에서 각 레코드는 **timestamp(시간)**과 함께 전송됨

---

## 10. 이벤트 흐름 이해
- Kafka는 **이벤트 기반 시스템**으로 실시간 데이터를 수집하고 처리하는 데 사용됨  
- **이벤트 = 메시지 = 데이터 = 레코드 = 로그**

### ① Kafka의 이벤트 처리 흐름
- **이벤트 소스(Event Source)**: 데이터를 생성하는 출처
- **이벤트 스트림(Event Stream)**: 이벤트들이 순서대로 쌓인 스트림  
  - 토픽(Topic)에 연속된 레코드들이 쌓여 스트림을 형성
- **이벤트 처리 어플리케이션(Event Processing Application)**:  
  - 이벤트를 처리하는 애플리케이션 (예: 주문 이벤트를 받아 결제 시스템으로 전달하거나 고객에게 알림을 보냄)

### ② Key/Value의 역할
- Kafka에서는 레코드를 **Key/Value** 구조로 저장  
- 그러나, 이 **Key**는 데이터베이스 조회를 위해 사용되지 않음

#### Key의 주요 역할
- **Ordering**: 레코드의 순서를 유지  
- **Colocating**: 같은 키를 가진 레코드를 동일한 파티션에 배치  
  - 관련된 데이터가 항상 같은 파티션에 저장되기 때문에 효율적인 소비와 처리 가능

#### Compaction(압축)
- Kafka는 모든 레코드를 **Append-only 방식**으로 저장.  
  - 즉, 레코드가 계속 추가되기만 하며 기본적으로 삭제되지 않음  
- **Compaction**: 특정 키에 대한 중복된 레코드 중 **최신 데이터**만 남기고 나머지를 삭제하는 기능  
  - 예: 사용자의 프로필 데이터가 여러 번 업데이트될 때, 가장 최신의 상태만 남기고 이전 상태를 삭제 (상태 관리에 유용)

---

# 11. Kafka Connect - Connectors

- **Kafka Connect**: Confluent Platform에서 데이터의 송수신을 위해 **Data Source(데이터 소스)** 및 **Data Sink(데이터 싱크)** 시스템과 쉽게 연결할 수 있도록 지원하는 기능
- Confluent는 외부 시스템과 Kafka를 연결할 수 있는 다양한 커넥터를 제공하여 여러 시스템 간의 **데이터 송수신**을 쉽게 처리할 수 있다.
- **Connector Hub**: 커넥터를 쉽게 검색하고 사용할 수 있는 플랫폼
  - **Source**: 데이터를 Kafka로 전송하는 시스템
  - **Sink**: Kafka 데이터를 외부 시스템으로 보내는 시스템

---

# 12. Schema Registry / Schema Validation

- Kafka에서 데이터를 주고받을 때 **Schema Registry**와 **Schema Validation**은 데이터의 형식을 관리하고 일관성을 유지
- Confluent Platform은 중앙 저장소인 **Schema Registry**를 제공하며, 이를 통해 **표준 스키마**를 정의하고 모든 애플리케이션이 동일한 스키마를 준수하는지 확인

## ① 애플리케이션 개발 호환성 보장 – Schema Registry
- 여러 애플리케이션이 **Schema Registry**에 접근하여 동일한 **스키마와 버전**을 사용
- 스키마는 **Serializer**에 의해 직렬화되어 Kafka Topic에 전송되기 전 **Schema Registry**와 동기화된다.

![Schema Registry 이미지](./images/Schema_Registry.PNG)

## ② 운영 환경에서 안전하게 배포 – Schema Validation
- Kafka의 **Broker**는 Schema Registry와 통합되어 클라이언트가 전송하는 데이터가 사전에 정의된 **스키마와 일치**하는지 **검증**
- 이 기능을 통해 **잘못된 스키마**를 사용하는 경우 즉시 **오류가 발생**하며 **데이터 전송이 차단**된다.

![Schema Validation 이미지](./images/Schema_Validation.PNG)